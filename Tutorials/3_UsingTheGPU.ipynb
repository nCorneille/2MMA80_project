{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62280cbf-2048-4de7-a4bd-09e121b2ce79",
   "metadata": {},
   "source": [
    "# Using the GPU\n",
    "\n",
    "The leveraging of GPUs to train neural networks has been a key ingedient in their current success. GPUs were primarily developed for rendering 3D graphics. Dealing with 3D geometry consists mainly of doing linear algebra on a very large scale. Hence, any problem that requires that type of computations can benefit from a GPU as well.\n",
    "\n",
    "In this tutorial we will go over how we can use PyTorch to leverage our GPU. Lets start with the usual imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f4cbb47-3f27-4a5a-ba8a-53d2f127d09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import math, os, time\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# progress bars\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862ca7c2-91c5-4f93-8060-982113234460",
   "metadata": {},
   "source": [
    "PyTorch accesses Nvidia GPUs via the CUDA API, hence in PyTorch these are refered to as CUDA devices. Lets check if we have a CUDA device available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "924f6717-5a48-4ae3-8f3c-001bcb27a648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f37e80-846e-473a-b563-fcf75eabc40f",
   "metadata": {},
   "source": [
    "## CPU\n",
    "\n",
    "Lets set up a big compute task, we are going to do some computations with two large tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "941295be-abb2-4592-8abb-f94b3ff64970",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10,1000,1000)\n",
    "b = torch.rand(10,1000,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7486b9d9-131b-4a10-83d7-19152f4f5ed6",
   "metadata": {},
   "source": [
    "Next to their shape and dtype, tensors have a third important property: their device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfb45fc2-1e2a-4228-ba2d-9c8aa7de8b2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ebf74b2-ff53-4788-8b9d-16a97165ea87",
   "metadata": {},
   "source": [
    "If a tensor's `device` is `cpu` that means the data in the tensor is resident in your computer's main memory and that any computations you do with it will be performed by the CPU.\n",
    "\n",
    "Let us time how long it takes the CPU to add up these two tensors. For that we can use the `%%timeit` magic cell function, starting a cell with `%%timeit` will cause python to benchmark the cell. The options `-r` and `-n` specify the number of runs respectively loops per run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef8b62f6-4959-49d0-a4e3-a068887e692e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.3 ms ± 617 µs per loop (mean ± std. dev. of 5 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r5 -n50\n",
    "\n",
    "c = a + b**2 + a.sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41a178f-c2f1-4943-bc89-3dee496f622a",
   "metadata": {},
   "source": [
    "The code will report the average running time of the cell. The duration will be heavily dependend on your hardware."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c3b3af-cab2-4bbe-b812-f02166046182",
   "metadata": {},
   "source": [
    "## CUDA\n",
    "\n",
    "Lets see how we can make this faster by doing the computation on the GPU. The first problem we have is that our tensors live in our computer's main memory, memory that our GPU does not have direct access to. A GPU comes with its own memory to work with, so we will need to move the tensors. We can copy a tensor over to the CUDA device by calling `.cuda()` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c25af807-d69e-4a98-b0a7-7c19104223e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "a_cu = a.cuda()\n",
    "b_cu = b.cuda()\n",
    "\n",
    "print(a_cu.device)\n",
    "print(b_cu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8992292d-64ee-4696-8cc6-b38cb1963d2a",
   "metadata": {},
   "source": [
    "The new tensors `a_cu` and `b_cu` are exact copies of `a` and `b` but live in the GPU's memory. We can check their `.device` property to verify what device they reside on. You can install more than one GPU in your computer, so `cuda:0` indicates the tensors reside on the 1st one. Now we can redo the addition but executed on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d47a5d8f-616e-4d3d-8741-25064c7686ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 5.91 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "54.7 µs ± 52.9 µs per loop (mean ± std. dev. of 5 runs, 50 loops each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -r5 -n50\n",
    "\n",
    "c_cu = a_cu + b_cu**2 + a_cu.sin()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e641e0-dcad-4de3-9b52-3af89590cd1a",
   "metadata": {},
   "source": [
    "Compare the running time of this cell with the previous CPU version, how much faster it is depends on your hardware but expect 10x-100x."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43795cc8-65c1-4ffa-ad09-b33148a351fe",
   "metadata": {},
   "source": [
    "The result of an operation on tensors on a certain device will also end up on that device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2176d6bc-b362-4831-b4c4-23299ddbea74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "c = a + b\n",
    "c_cu = a_cu + b_cu\n",
    "\n",
    "print(c.device)\n",
    "print(c_cu.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6f546-e34f-4795-b7a1-04c4729f6748",
   "metadata": {},
   "source": [
    "Lets compare whether the CPU and GPU yielded the same answer. Asking the CPU and GPU to yield the exact same answer is too much. Particularly for non-trivial functions such as trigonometric functions the answer depends on the choice of algorithm. Instead we will contend ourselves with checking whether all the entries are sufficiently close. The `.allclose()` function will return `True` if all entries are equal within tolerance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f0f08c9-e153-47d4-bda4-5def315c9de2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\BARTSM~1\\AppData\\Local\\Temp/ipykernel_21836/2512556928.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_cu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!"
     ]
    }
   ],
   "source": [
    "c.allclose(c_cu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b1713a-b791-421d-99e6-0190219ac15b",
   "metadata": {},
   "source": [
    "This has throw an exception because we cannot compare tensors located on different devices. Copying a tensor on the GPU back to main memory is accomplished by calling `.cpu()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e91209d5-6174-4655-8976-67b508bfc361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.allclose(c_cu.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5091ff-d1a2-463d-b6ef-7739b1494012",
   "metadata": {},
   "source": [
    "This version works since both tensors are in main memory and the CPU can perform the comparison. Fortunately the CPU and GPU have arrived at the same answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8baa4af-27f0-49ac-af73-838c0617d686",
   "metadata": {},
   "source": [
    "## Creating Tensors on the GPU\n",
    "\n",
    "It is not necessary to shuffle tensors in between the CPU and GPU all the time, in fact this takes a lot of time and should be avoided. All the ways of creating tensors we saw in the previous tutorial can be used to create tensors on a CUDA device directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5ee7e7b-c070-405a-8e5f-222fc3a8a8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2.],\n",
       "         [3., 4.]], device='cuda:0'),\n",
       " tensor([[1., 1.],\n",
       "         [1., 1.]], device='cuda:0'),\n",
       " tensor([[0.0667, 0.1461, 0.0265],\n",
       "         [0.7871, 0.3133, 0.3860]], device='cuda:0'))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda = torch.device('cuda') # select the default CUDA device, normally cuda:0\n",
    "\n",
    "d = torch.tensor([[1.,2.],[3.,4.]], device=cuda) # create this tensor directly on `device`\n",
    "e = torch.ones((2, 2), device=cuda)\n",
    "f = torch.rand((2, 3), device=cuda)\n",
    "\n",
    "d, e, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e08066-1b89-4dfe-a59d-a0b93290065b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Optional Exercises\n",
    "\n",
    "1. Use `%%timeit` to verify that generating a large random matrix on the CPU and then copying it to the GPU is slower than generating is directly on the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f91f9f-0677-46e6-ac01-0f1865fa33c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mathnn] *",
   "language": "python",
   "name": "conda-env-mathnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
