{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06d6d6c0-13d2-4a64-b059-db201b3d5124",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "In this notebook we are going to learn about the open source machine learning framework [PyTorch](https://pytorch.org/). There are many machine learning framework of varying popularity and functionality, such as [TensorFlow](https://www.tensorflow.org/) and [MXNet](https://mxnet.apache.org/). We will be using PyTorch because it is flexible with a low barrier of entry. For those reasons it is extensively used in research with many papers being accompanied by PyTorch code. All machine learning frameworks are broadly similar, so learning one will allow you to easily pick up another one later.\n",
    "\n",
    "If you already worked with PyTorch, feel free to skip this tutorial.\n",
    "\n",
    "Lets start with importing all the stuff we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0511199-84a9-410f-a9e6-ce6171af95c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import math, os, time\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# progress bars\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "# PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18976724-8d3f-4540-a082-1c14db3c018d",
   "metadata": {},
   "source": [
    "### Reproducibility\n",
    "\n",
    "When we perform scientific experiments we should ensure others can reproduce those experiments. But that is a problem when we use stochastic functions. The workaround is fixing the seed of the random number generator so that everyone else gets the same sequence of random numbers. We can fix the RNG seed in PyTorch as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbf8e23a-3476-4977-a7e6-c9a97d77e23a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20cea1c2f50>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9814fc7a-2a33-49a1-b6ff-8093bf5949a3",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25de8da5-82e6-4ebb-bb4b-bf1250ee3cc7",
   "metadata": {},
   "source": [
    "Tensors in PyTorch are multi-dimensional arrays of numbers. They work the same as numpy arrays or numeric arrays in Mathematica but add functionality such as GPU and backpropagation support that we will need for neural networks. There are several ways to create tensors, the  most straightforward is calling `torch.empty` with the desired dimensions of the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c931b46-782d-4797-99bf-4fd915797785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.0102e-38, 9.0919e-39, 1.0102e-38, 8.9082e-39],\n",
       "         [8.4489e-39, 1.0102e-38, 1.0561e-38, 1.0010e-38],\n",
       "         [1.0653e-38, 1.0102e-38, 8.4490e-39, 9.6429e-39]],\n",
       "\n",
       "        [[8.4490e-39, 9.6429e-39, 9.2755e-39, 1.0286e-38],\n",
       "         [9.0919e-39, 8.9082e-39, 9.2755e-39, 8.4490e-39],\n",
       "         [1.0194e-38, 9.0919e-39, 8.4490e-39, 1.0653e-38]]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(2, 3, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c017465e-dde4-4d0e-869e-6e48c35df50e",
   "metadata": {},
   "source": [
    "You can see this has created a $2 \\times 3 \\times 4$ array, the dimensions of a tensor are called its __shape__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8668cf12-eae9-49a4-96ee-3bde61046b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "dim1=2, dim2=3, dim3=4\n"
     ]
    }
   ],
   "source": [
    "print(a.shape) # get the shape of a tensor\n",
    "dim1, dim2, dim3 = a.shape\n",
    "print(f\"dim1={dim1}, dim2={dim2}, dim3={dim3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bb1b36-2cc6-4820-9186-5ae4ae72cfa2",
   "metadata": {},
   "source": [
    "The reason the numbers in `a` are random is because calling `torch.empty` allocates memory for the tensor but does not initialize it. Meaning that whatever was left inside that part of the memory is now interpreted as the contents of the tensor.\n",
    "\n",
    "We can instead provide our own contents by passing a Python array to `torch.tensor`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc0d08c6-84cc-44cb-aff5-62d0bef7a392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2],\n",
       "        [3, 4]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.tensor([[1,2],[3,4]])\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90427c0-5f03-4e62-8c70-1d235dec50bf",
   "metadata": {},
   "source": [
    "PyTorch provides many other functions for creating new tensors, such as\n",
    "- `torch.zeros` : all values set to zero\n",
    "- `torch.ones` : all values set to 1\n",
    "- `torch.full` : all values set to a constant\n",
    "- `torch.rand` : random values in $[0,1]$\n",
    "- `torch.randn` : normal distributed values with mean 0 and variance 1\n",
    "- `torch.arange` : 1D tensor with values from an equidistant partition of an interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36443da6-26d8-4bf2-983b-a8bfc2d358c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c = \n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "d = \n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "e = \n",
      "tensor([[5., 5., 5.],\n",
      "        [5., 5., 5.]])\n",
      "f = \n",
      "tensor([[0.4963, 0.7682, 0.0885],\n",
      "        [0.1320, 0.3074, 0.6341]])\n",
      "g = \n",
      "tensor([[ 1.2645, -0.6874,  0.1604],\n",
      "        [-0.6065, -0.7831,  1.0622]])\n",
      "h = \n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "c = torch.zeros(2, 3)\n",
    "print(f\"c = \\n{c}\")\n",
    "\n",
    "d = torch.ones(2, 3)\n",
    "print(f\"d = \\n{d}\")\n",
    "\n",
    "e = torch.full((2, 3), 5.)\n",
    "print(f\"e = \\n{e}\")\n",
    "\n",
    "f = torch.rand(2, 3)\n",
    "print(f\"f = \\n{f}\")\n",
    "\n",
    "g = torch.randn(2, 3)\n",
    "print(f\"g = \\n{g}\")\n",
    "\n",
    "h = torch.arange(1, 10)\n",
    "print(f\"h = \\n{h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c34b59-d69f-43a8-8e31-30b9ff05b420",
   "metadata": {},
   "source": [
    "## Numpy Compatibility\n",
    "\n",
    "PyTorch tensors and Numpy arrays can be converted to each other. This is sometimes handy since many packages are built to handle Numpy arrays, we can use those packages with PyTorch aswell then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a197dfde-90c0-429b-bc43-af86b25bd489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], dtype=torch.float64)\n",
      "[[0.16102946 0.28226858]\n",
      " [0.68160856 0.915194  ]]\n"
     ]
    }
   ],
   "source": [
    "# Converting a Numpy array to a PyTorch tensor:\n",
    "numpy_array = np.array([[1.,2.],[3.,4.]])\n",
    "tensor = torch.from_numpy(numpy_array)\n",
    "\n",
    "print(tensor)\n",
    "\n",
    "# Converting a PyTorch tensor to a Numpy array:\n",
    "tensor = torch.rand(2, 2)\n",
    "numpy_array = tensor.numpy()\n",
    "\n",
    "print(numpy_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889206e4-216f-44d3-afa5-8c79c75ba7d5",
   "metadata": {},
   "source": [
    "## Operations\n",
    "\n",
    "Operating with tensors is straightforward, the usual arithmetic operators are supported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f43e8c91-9a20-442f-ab18-16a520ce9294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[100, 200],\n",
      "        [300, 400]])\n",
      "tensor([[0, 5],\n",
      "        [3, 5]])\n",
      "tensor([[-1,  6],\n",
      "        [ 0,  4]])\n",
      "tensor([[1, 8],\n",
      "        [1, 4]])\n",
      "tensor([[ 2, -1],\n",
      "        [ 3,  3]])\n",
      "tensor([[-1.0000,  0.6667],\n",
      "        [    inf,  4.0000]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2],[3,4]])\n",
    "b = torch.tensor([[-1,3],[0,1]])\n",
    "print(100*a)\n",
    "print(a+b)\n",
    "print(a*b)\n",
    "print(a**b)\n",
    "print(a-b)\n",
    "print(a/b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793f2af7-cd62-4da2-8363-9ab55c4a2e6d",
   "metadata": {},
   "source": [
    "PyTorch contains a large collection of operations implementing common mathematical functions, see <https://pytorch.org/docs/stable/tensors.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "004a46b9-168c-4d0b-a7f3-fc054205223e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8415,  0.9093],\n",
      "        [ 0.1411, -0.7568]])\n",
      "tensor([[ 0.5403, -0.4161],\n",
      "        [-0.9900, -0.6536]])\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n",
      "tensor([[1, 3],\n",
      "        [0, 1]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "# We can pass the tensor to an operator:\n",
    "print(torch.sin(a))\n",
    "print(torch.cos(a))\n",
    "print(torch.sqrt(a))\n",
    "print(torch.abs(b))\n",
    "print(torch.t(a)) # transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "979d8923-e385-4d9a-8821-171b8d55435b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8415,  0.9093],\n",
      "        [ 0.1411, -0.7568]])\n",
      "tensor([[ 0.5403, -0.4161],\n",
      "        [-0.9900, -0.6536]])\n",
      "tensor([[1.0000, 1.4142],\n",
      "        [1.7321, 2.0000]])\n",
      "tensor([[1, 3],\n",
      "        [0, 1]])\n",
      "tensor([[1, 3],\n",
      "        [2, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Alternatively, we can call the operation on the tensor itself:\n",
    "print(a.sin())\n",
    "print(a.cos())\n",
    "print(a.sqrt())\n",
    "print(b.abs())\n",
    "print(a.t())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbca8c5-dd2a-4a11-82ec-c2a60726d555",
   "metadata": {},
   "source": [
    "## Reshaping & Selecting\n",
    "\n",
    "Sometimes is is necessary to change the shape of a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "448df540-ef47-4e29-8851-189db39a4774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(6)\n",
    "print(a)\n",
    "\n",
    "b = a.reshape(2, 3) # new shape\n",
    "print(b)\n",
    "\n",
    "c = a.reshape(-1, 2) # one dimension can be automatically infered by specifying it as -1\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79959a58-8086-4cdb-aa48-f0244fca5d48",
   "metadata": {},
   "source": [
    "Selections in tensors can be made in the same way as in Numpy, indexing starts at 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842bab66-ca1f-496d-a101-c5ab2d76c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 3, 4])\n",
      "tensor([0, 1, 2])\n",
      "tensor([5])\n"
     ]
    }
   ],
   "source": [
    "print(a[2:5]) # items [2, 5) in the first dimension\n",
    "print(b[0,:]) # first item of the first dimension, all in the second dimension\n",
    "print(c[-1,1:]) # last item of the first dimensions, all except the first in the second dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5573795-4c80-4d0b-9589-bf1e7ce547b1",
   "metadata": {},
   "source": [
    "## Datatypes\n",
    "\n",
    "Another important property of a tensor is its datatype, or _dtype_. To ensure performance, tensors only handle native numeric types. The default datatype of a tensor is _single-precision floating-point_, refered to in PyTorch as `torch.float32` since it occupies 32 bits. We will do most if not all our computations in single-precision floating-point format. The reason we do not use double-precision floating point (like in most scientific computing work) is because we simply do not need that level of precision. The added memory capacity and bandwith needed to handle 64-bit floating point numbers is another reason to stick with 32-bit floats. In fact modern GPUs support various 16-bit floating point formats (`torch.float16`, `torch.bfloat16`) specifically for deep learning. But since 16-bit support is not universal we will stick to 32-bit floats for our work.\n",
    "\n",
    "More information can be found at <https://pytorch.org/docs/stable/tensor_attributes.html>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8edc9a6f-b19a-4e3e-a509-c0b245689093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.int64\n",
      "torch.float32\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(2, 3, 4) # float32 is the default\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.arange(4) # yields int64 by default\n",
    "print(b.dtype)\n",
    "\n",
    "c = a + b # operations with different dtypes casts to the 'largest' dtype\n",
    "print(c.dtype)\n",
    "\n",
    "# we can also explicitly cast to another dtype:\n",
    "d = b.to(torch.float64)\n",
    "print(d.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4371ca3-d185-4ebf-bfad-17699b38da7f",
   "metadata": {},
   "source": [
    "## Broadcasting\n",
    "\n",
    "Broadcasting is what happens when we do operations on tensors with different shapes. Subject to certain restrictions, the smaller tensor will be _broadcast_ to match the size of the larger one, this happens without physically copying data. Broadcasting in PyTorch works the same as in Numpy, see <https://pytorch.org/docs/stable/notes/broadcasting.html> for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "413970d5-f436-440d-a099-53ae65ecc5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2, 3, 4],\n",
      "        [6, 7, 8]])\n",
      "tensor([[0, 0, 0],\n",
      "        [3, 3, 3]])\n",
      "tensor([[-10, -20, -30],\n",
      "        [-40, -50, -60]])\n",
      "tensor([[ 0, -1, -2],\n",
      "        [ 1,  0, -1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3],[4,5,6]])\n",
    "b = torch.tensor([[1],[2]])\n",
    "c = torch.tensor([-1,-2,-3])\n",
    "d = torch.tensor([-10])\n",
    "\n",
    "# shape of a = [ 2 , 3 ]\n",
    "# shape of b = [ 2 , 1 ]\n",
    "# shape of c = [     3 ]\n",
    "# shape of d = [     1 ]\n",
    "\n",
    "print(a+b)\n",
    "print(a+c)\n",
    "print(a*d)\n",
    "print(b+c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138d24aa-7801-4237-af10-c08c28deabc3",
   "metadata": {},
   "source": [
    "## Gradients\n",
    "\n",
    "So far we have done nothing with PyTorch that we cannot do with Numpy or any other scientific computing program. What distinguishes PyTorch is its support for calculating gradients automatically. We have to let PyTorch know which tensors we want to calculate gradients for, we do this by setting `requires_grad=True`. We can pass `requires_grad=True` during construction of a new tensor or call `.requires_grad(True)` on an existing tensor.\n",
    "\n",
    "PyTorch implements _Autograd_ to computate gradients, which is distinct from finite differences or symbolic derivatives. Autograd will only give us the gradient at one particular point and will only do so after having compute the function at that point. First evaluating the function at our point of interest is called the _forward pass_, let us do that for a simple function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cb1dc3f-50ae-461f-99b5-638d0f3c34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5.], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# set up our tensors and indicate which we want to compute the gradient for (just the derivative in 1D)\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "a = torch.tensor([2.0])\n",
    "b = torch.tensor([3.0])\n",
    "\n",
    "# execute the forward pass\n",
    "y = a*x + b\n",
    "\n",
    "# lets see whats in y\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21d8aa9-a75f-4d0c-8fb6-5b1ae66a0fdc",
   "metadata": {},
   "source": [
    "As expected `y` contains the right answer $2*1+3=5$. But it contains an additional field `grad_fn=<AddBackward0>`. When we did a computation with a tensor that has `requires_grad=True`, PyTorch tracks everything we do with that tensor so that it can later do backpropagation to calculate the gradient. The `grad_fn` property is part of the mechanism, the details of this mechanism are not important for us to understand (if you are interest: see <https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html>) but we can now use backpropagation to calculate $\\frac{\\partial y}{\\partial x}(1)$.\n",
    "\n",
    "We execute the backward pass by calling `.backward()` on the output tensor `y`, afterward we can look up the gradient of `x` by accessing `x.grad`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29a5b24f-c24a-4f6e-a9b8-880c36192f3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.])\n"
     ]
    }
   ],
   "source": [
    "# execute the backward pass\n",
    "y.backward()\n",
    "\n",
    "# gradient of y w.r.t. to x:\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cee921-6567-421c-ac07-935b6a5b1914",
   "metadata": {},
   "source": [
    "Indeed $\\frac{\\partial y}{\\partial x}(1)=2$.\n",
    "\n",
    "Let us do something a little more complicated:\n",
    "$$\n",
    "    y = - x_1^2 + 0\\, x_2^2 + 2\\, x_3^2 .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75053323-d6ca-4b6d-b53c-9a8568f05e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(17., grad_fn=<SumBackward0>)\n",
      "tensor([-2.,  0., 12.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "a = torch.tensor([-1.0, 0.0, 2.0])\n",
    "\n",
    "y = (a*(x**2)).sum()\n",
    "print(y)\n",
    "\n",
    "y.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0332579c-3efb-4bb4-aad0-2105f643ffe8",
   "metadata": {},
   "source": [
    "You can verify that this is indeed the correct answer.\n",
    "\n",
    "Implementing gradient descent would now come down to doing `x = x - learning_rate * x.grad`. We will not have to do this manually as PyTorch has optimizers that handle all these details but it is good to know what is happening when we are running an optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mathnn] *",
   "language": "python",
   "name": "conda-env-mathnn-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
